{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Application III: Comparing Classifiers\n",
    "\n",
    "**Overview**: In this practical application, your goal is to compare the performance of the classifiers we encountered in this section, namely K Nearest Neighbor, Logistic Regression, Decision Trees, and Support Vector Machines.  We will utilize a dataset related to marketing bank products over the telephone.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "Our dataset comes from the UCI Machine Learning repository [link](https://archive.ics.uci.edu/ml/datasets/bank+marketing).  The data is from a Portugese banking institution and is a collection of the results of multiple marketing campaigns.  We will make use of the article accompanying the dataset [here](CRISP-DM-BANK.pdf) for more information on the data and features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Understanding the Data\n",
    "\n",
    "To gain a better understanding of the data, please read the information provided in the UCI link above, and examine the **Materials and Methods** section of the paper.  How many marketing campaigns does this data represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the Materials and Method : \n",
    "- GOAL : Goal was to increase efficiency of directed campaigns for long-term deposit subscriptions by reducing the number of contacts to do.\n",
    "- the Lift is the most commonly used metric to evaluate prediction models (Coppock 2002). In particular, the cumulative Lift curve is a percentage graph that divides the population into deciles, in which population members are placed based on their predicted probability of response. The responder deciles are sorted, with the highest responders are put on the first decile.\n",
    "- 17 Campaigns between May 2008 and Nov 2010. Total of 79354 contacts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Read in the Data\n",
    "\n",
    "Use pandas to read in the dataset `bank-additional-full.csv` and assign to a meaningful variable name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/bank-additional-full.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['campaign'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Understanding the Features\n",
    "\n",
    "\n",
    "Examine the data description below, and determine if any of the features are missing values or need to be coerced to a different data type.\n",
    "\n",
    "\n",
    "```\n",
    "Input variables:\n",
    "# bank client data:\n",
    "1 - age (numeric)\n",
    "2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n",
    "3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
    "4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n",
    "5 - default: has credit in default? (categorical: 'no','yes','unknown')\n",
    "6 - housing: has housing loan? (categorical: 'no','yes','unknown')\n",
    "7 - loan: has personal loan? (categorical: 'no','yes','unknown')\n",
    "# related with the last contact of the current campaign:\n",
    "8 - contact: contact communication type (categorical: 'cellular','telephone')\n",
    "9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
    "10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n",
    "11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "# other attributes:\n",
    "12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
    "14 - previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n",
    "# social and economic context attributes\n",
    "16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)\n",
    "17 - cons.price.idx: consumer price index - monthly indicator (numeric)\n",
    "18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)\n",
    "19 - euribor3m: euribor 3 month rate - daily indicator (numeric)\n",
    "20 - nr.employed: number of employees - quarterly indicator (numeric)\n",
    "\n",
    "Output variable (desired target):\n",
    "21 - y - has the client subscribed a term deposit? (binary: 'yes','no')\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['education'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Understanding the Task\n",
    "\n",
    "After examining the description and data, your goal now is to clearly state the *Business Objective* of the task.  State the objective below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "Increase efficiency of directed campaigns for long term deposit subscriptions by optimizing the number of contacts made.\n",
    "\n",
    "The result is not to specify the efficiency % or what the optimal / minimum # of contacts are, but more to identify which customer profile / type is better suited for the compaign.\n",
    "\n",
    "Target Outcome (Dependent variable) : Is represented as 'y'. Value 'yes' indicates cient has subscribed to long term deposity. 'no' is the opposite\n",
    "Independent Variables : Various data elements captured as part of the campaign as well as from internal bank records are listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportion of yes and no in the target variable\n",
    "df['y'].value_counts()\n",
    "# --- NOTE : Output clearly shows Target variable is imbalanced and hence we may need to stratify our sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5: Engineering Features\n",
    "\n",
    "Now that you understand your business objective, we will build a basic model to get started.  Before we can do this, we must work to encode the data.  Using just the bank information features, prepare the features and target column for modeling with appropriate encoding and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try forcing Python to identify the data types\n",
    "df = df.convert_dtypes()\n",
    "print(\"Education count= \", df['education'].value_counts())\n",
    "# --- Rnning above converted Object to String. Hence explicitly converting to numeric\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -- Create X and Y and split the data before Feature encoding / engineering.\n",
    "# -- NOTE : As stated above in \"Understandding the Features\" and also from above pairplot we see a strong correlation between duration and y.\n",
    "# Hence lets remove the column for prediction purposes.\n",
    "X = df.drop(columns=['emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed', 'duration', 'y'])\n",
    "y = df['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "\n",
    "# --NOTE : Need to convert target var as well else JamesStein does not like it and errors \"AttributeError: 'numpy.ndarray' object has no attribute 'groupby'\"\n",
    "y_train_enc = pd.get_dummies(y_train, drop_first=True, prefix='y')\n",
    "y_test_enc = pd.get_dummies(y_test, drop_first=True, prefix='y')\n",
    "\n",
    "# ---- boolean data : Use Dummy Encoder - Contact\n",
    "contact_train_enc = pd.get_dummies(X_train['contact'], drop_first=True, prefix='contact')\n",
    "contact_test_enc = pd.get_dummies(X_test['contact'], drop_first=True, prefix='contact')\n",
    "X_train_enc = pd.concat([X_train, contact_train_enc], axis=1)\n",
    "X_test_enc = pd.concat([X_test, contact_test_enc], axis=1)\n",
    "X_train_enc = X_train_enc.drop(columns=['contact'])\n",
    "X_test_enc = X_test_enc.drop(columns=['contact'])\n",
    "\n",
    "\n",
    "# --- Ordinal encoder for\n",
    "# education : 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown'\n",
    "# Given 'unknown' is a tricky value, we use the avg value to fill this void. Avg is calculated using Ordinal values of other values\n",
    "# Once we run the encoder without 'unknown', the value is eliminated from the data set, unfortunately. Hence running it as one operation.\n",
    "education_map = {'basic.4y':4,'basic.6y':6,'basic.9y':9,'high.school':12,'illiterate':-1,'professional.course':14,'university.degree':16, 'unknown':11}\n",
    "X_train_enc['education'] = X_train_enc['education'].map(education_map)\n",
    "X_test_enc['education'] = X_test_enc['education'].map(education_map)\n",
    "\n",
    "# unknown_val = int(X_test_enc['education'].mean())\n",
    "# ed_all_map = {4:4, 6:6, 9:9, 12:12, -1:-1, 14:14, 16:16, 'unknown':unknown_val}\n",
    "# X_train_enc['education'] = X_train_enc['education'].map(ed_all_map)\n",
    "# numerical_cols = ['age', 'campaign', 'pdays', 'previous', 'education']\n",
    "# std_scaler = StandardScaler()\n",
    "# X_train_enc = std_scaler.fit_transform(X_train_enc[numerical_cols])\n",
    "# X_test_enc = std_scaler.transform(X_test_enc[numerical_cols])\n",
    "\n",
    "#Print unique values for each column\n",
    "categorical_cols = ['job', 'marital', 'default', 'housing', 'loan', 'month', 'day_of_week', 'poutcome'] #, 'contact', 'education']\n",
    "\n",
    "\n",
    "# James Stein encoder - job, marital, housing, loan, month, day_of_week, campaign, pdays, previous, poutcome\n",
    "js_encoder = ce.JamesSteinEncoder(cols=categorical_cols, random_state=42)\n",
    "js_encoder.fit(X_train_enc, y_train_enc)\n",
    "X_train_enc = js_encoder.transform(X_train_enc)\n",
    "X_test_enc = js_encoder.transform(X_test_enc)\n",
    "\n",
    "X_train_enc.info()\n",
    "print(y_train_enc.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- encoding using Label Encoder and Target Encoder\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.discriminant_analysis import StandardScaler #TODO : Shud hv used sklearn.preprocessing.StandardScaler here.\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_target_enc = le.fit_transform(y_train)\n",
    "y_test_target_enc = le.transform(y_test)\n",
    "\n",
    "categorical_cols = ['job', 'marital', 'default', 'housing', 'loan', 'month', 'day_of_week', 'poutcome', 'contact', 'education']\n",
    "numerical_cols = ['age', 'campaign', 'pdays', 'previous']\n",
    "col_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('categorical', TargetEncoder(), categorical_cols),\n",
    "        #('target', LabelEncoder(), 'y')\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(type(y_train_target_enc))\n",
    "X_train_target_enc = col_transformer.fit_transform(X_train, y_train_target_enc)\n",
    "X_test_target_enc = col_transformer.transform(X_test)\n",
    "\n",
    "cat_col_names = col_transformer.named_transformers_['categorical'].get_feature_names_out(categorical_cols)\n",
    "all_col_names = list(numerical_cols) + list(cat_col_names)\n",
    "\n",
    "X_train_target_enc_df = pd.DataFrame(X_train_target_enc, columns=all_col_names)\n",
    "X_test_target_enc_df = pd.DataFrame(X_test_target_enc, columns=all_col_names)\n",
    "\n",
    "print(X_train_target_enc_df.info())\n",
    "X_train_target_enc_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6: Train/Test Split\n",
    "\n",
    "With your data prepared, split it into a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given its a good practice to split Train, Test data before Feature engineering, this step has been performed above at the beginning of feature engineering step.\n",
    "\n",
    "y_train_enc = y_train_enc.to_numpy()\n",
    "y_test_enc = y_test_enc.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7: A Baseline Model\n",
    "\n",
    "Before we build our first model, we want to establish a baseline.  What is the baseline performance that our classifier should aim to beat?\n",
    "\n",
    "For this we use the DummyClassifier as a baseline, a model that predicts using basic rules and not necessarily from learning from the data. Other models we use must beat this model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "start_time = time.time()\n",
    "dummy_classifier = DummyClassifier(strategy='stratified', random_state=42)\n",
    "dummy_classifier.fit(X_train_enc, y_train_enc)\n",
    "# X_train_transform = dummy_classifier.transform(X_train_enc)\n",
    "y_test_preds = dummy_classifier.predict(X_test_enc)\n",
    "end_time = time.time()\n",
    "\n",
    "dummy_accuracy = accuracy_score(y_test_enc, y_test_preds)\n",
    "dummy_precision = precision_score(y_test_enc, y_test_preds)\n",
    "dummy_recall = recall_score(y_test_enc, y_test_preds)\n",
    "dummy_f1score = f1_score(y_test_enc, y_test_preds)\n",
    "dummy_conf_matrix = confusion_matrix(y_test_enc, y_test_preds)\n",
    "\n",
    "print(\"Total time taken by Dummy classifier: \", end_time - start_time)\n",
    "print('Dummy Classifier scores : \\n'\n",
    "      f'Accuracy: {dummy_accuracy}\\n'\n",
    "      f'Precision: {dummy_precision}\\n'\n",
    "      f'Recall: {dummy_recall}\\n'\n",
    "      f'F1 score: {dummy_f1score}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings : \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "The baseline model (Dummy Classifer) returns an Accuracy rate of ~80%, which is what we would expect our more sophisticated models to beat.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cProfile import label\n",
    "# import plotly.express as px\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "cmd = ConfusionMatrixDisplay(dummy_conf_matrix, display_labels=['not enrolled','enrolled'])\n",
    "cmd.plot()\n",
    "\n",
    "# dummy_conf_matrix_plot = ConfusionMatrixDisplay(dummy_conf_matrix)\n",
    "# fig = px.imshow(dummy_conf_matrix)\n",
    "# fig.update_layout()\n",
    "# fig.update_xaxes(side='bottom') # move x axes tick labels to bottom\n",
    "# fig.update_traces(text=dummy_conf_matrix, texttemplate=\"%{text}\")\n",
    "# fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8: A Simple Model\n",
    "\n",
    "Use Logistic Regression to build a basic model on your data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "results = {}\n",
    "y_train_enc.ravel()\n",
    "y_test_enc.ravel()\n",
    "\n",
    "le_start_time = time.time()\n",
    "le_gridSearch = GridSearchCV(estimator = LogisticRegression(random_state=42),\n",
    "                             param_grid={'C': [0.01, 0.1, 1, 10, 100]}\n",
    "                             )\n",
    "le_gridSearch.fit(X_train_enc, y_train_enc)\n",
    "le_y_enc_pred = le_gridSearch.predict(X_test_enc)\n",
    "le_end_time = time.time()\n",
    "\n",
    "le_accuracy = accuracy_score(y_test_enc, le_y_enc_pred)\n",
    "le_precision = precision_score(y_test_enc, le_y_enc_pred)\n",
    "le_recall = recall_score(y_test_enc, le_y_enc_pred)\n",
    "le_f1score = f1_score(y_test_enc, le_y_enc_pred)\n",
    "le_conf_matrix = confusion_matrix(y_test_enc, le_y_enc_pred)\n",
    "\n",
    "results['LogisticRegression'] = {\n",
    "    'Accuracy Score': le_accuracy,\n",
    "    'Precision Score': le_precision,\n",
    "    'Recall': le_recall,\n",
    "    'F1 Score': le_f1score,\n",
    "    'Confusion Matrix': le_conf_matrix,\n",
    "    'Best Params': le_gridSearch.best_estimator_,\n",
    "    'Time Taken': (le_end_time - le_start_time)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Logistic Regression Accuracy score: {results['LogisticRegression']['Accuracy Score']}\")\n",
    "cmd = ConfusionMatrixDisplay(le_conf_matrix, display_labels=['not enrolled','enrolled'])\n",
    "cmd.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9: Score the Model\n",
    "\n",
    "What is the accuracy of your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings : \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "From an Accuracy standpoint the Logistic Regression provides close to 90% accuracy, which is much better than our baseline model's (Dummy) accuracy score of 80%. Hence LogisticRegression can be considered as a viable model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 10: Model Comparisons\n",
    "\n",
    "Now, we aim to compare the performance of the Logistic Regression model to our KNN algorithm, Decision Tree, and SVM models.  Using the default settings for each of the models, fit and score each.  Also, be sure to compare the fit time of each of the models.  Present your findings in a `DataFrame` similar to that below:\n",
    "\n",
    "| Model | Train Time | Train Accuracy | Test Accuracy |\n",
    "| ----- | ---------- | -------------  | -----------   |\n",
    "|     |    |.     |.     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Declare models and their params\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "models = {\n",
    "    'knn' : {\n",
    "        'model': KNeighborsClassifier(),\n",
    "        'params': {\n",
    "            'n_neighbors': range(1, 10)\n",
    "        }\n",
    "    },\n",
    "    'decision_tree': {\n",
    "        'model': DecisionTreeClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'max_depth': range(1, 10)\n",
    "        }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model': SVC(coef0=1, random_state=42),\n",
    "        'params': {\n",
    "            'gamma': [0.1, 1.0, 10.0, 100.0]\n",
    "        }\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the models without hyper param tuning\n",
    "\n",
    "basic_results = {}\n",
    "for model_name, model_params in models.items():\n",
    "    model = model_params['model']\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.fit(X_train_enc, y_train_enc)\n",
    "\n",
    "    y_train_preds = model.predict(X_train_enc)\n",
    "    y_test_preds = model.predict(X_test_enc)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    basic_results[model_name] = {\n",
    "        'Train Time': (end_time - start_time),\n",
    "        'Train Accuracy': accuracy_score(y_train_enc, y_train_preds),\n",
    "        'Test Accuracy' : accuracy_score(y_test_enc, y_test_preds)\n",
    "    }\n",
    "\n",
    "basic_results_df = pd.DataFrame(basic_results).T.sort_values(by='Test Accuracy', ascending=False)\n",
    "basic_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings / Result :\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "For comparison, we will primarily focus on Test Accuracy since this param essentially tells us how the model would perform on unseen data.\n",
    "\n",
    "All the above models did significantly better than the Dummy Classifier. But in terms of Accuracy, LogisticRegression with hyper parameter tuning did better than any of these 3 models which were not tuned. So may not be apples to apples comparison. \n",
    "\n",
    "But in terms of time, Decision Tree took the minimal time while its Test accuracy was the lowest. For a good balance between Train time and Test Accuracy, \"knn\" model seems to be the best option given the Accuracy is almost same as SVM but takes approximately one-twelth the time of SVM model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 11: Improving the Model\n",
    "\n",
    "Now that we have some basic models on the board, we want to try to improve these.  Below, we list a few things to explore in this pursuit.\n",
    "\n",
    "- More feature engineering and exploration.  For example, should we keep the gender feature?  Why or why not?\n",
    "- Hyperparameter tuning and grid search.  All of our models have additional hyperparameters to tune and explore.  For example the number of neighbors in KNN or the maximum depth of a Decision Tree.  \n",
    "- Adjust your performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - check the correlation among the features - Categorical vars encoded using JamesStein\n",
    "X_train_enc.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- check the correlation among features - Categorical vars encoded using Target Encoder\n",
    "X_train_target_enc_df.corr()\n",
    "\n",
    "# -- NOTE : Correlation between the 2 tables (James Stein encoded and Target Encoded) are largely similar.\n",
    "# One diff is Education.  In former cases, where Ordinal Encoder was used with values manually assigned to each unique value, corr = -0.1786\n",
    "# In case of latter, where Target Encoder was used to encode the values, corr = -0.013653."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Label and Target Encoders for Categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model using Label and Target Encoded features\n",
    "target_results = {}\n",
    "for model_name, model_params in models.items():\n",
    "    model = model_params['model']\n",
    "    start_time = time.time()\n",
    "\n",
    "    # gsearch = GridSearchCV(estimator=model,\n",
    "    #                        scoring='accuracy',\n",
    "    #                        cv=5,\n",
    "    #                        verbose=1)\n",
    "    model.fit(X_train_target_enc, y_train_target_enc)\n",
    "\n",
    "    y_train_preds = model.predict(X_train_target_enc)\n",
    "    y_test_preds = model.predict(X_test_target_enc)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    target_results[model_name] = {\n",
    "        'Train Time': (end_time - start_time),\n",
    "        'Train Accuracy': accuracy_score(y_train_enc, y_train_preds),\n",
    "        'Test Accuracy' : accuracy_score(y_test_enc, y_test_preds)\n",
    "    }\n",
    "\n",
    "target_results_df = pd.DataFrame(target_results).T.sort_values(by='Test Accuracy', ascending=False)\n",
    "target_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Comparing above against previous run, Decision Tree Test Accuracy improved marginally, else the numbers are pretty similar \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running for features encoded using Label Encoder and Target Encoder.\n",
    "tune_results = {}\n",
    "for model_name, model_params in models.items():\n",
    "    model = model_params['model']\n",
    "    start_time = time.time()\n",
    "\n",
    "    gsearch = GridSearchCV(estimator=model,\n",
    "                           param_grid=model_params['params'],\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=1)\n",
    "    gsearch.fit(X_train_target_enc, y_train_target_enc)\n",
    "\n",
    "    y_train_preds = gsearch.predict(X_train_target_enc)\n",
    "    y_test_preds = gsearch.predict(X_test_target_enc)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    tune_results[model_name] = {\n",
    "        'Train Time': (end_time - start_time),\n",
    "        'Train Accuracy': accuracy_score(y_train_enc, y_train_preds),\n",
    "        'Test Accuracy' : accuracy_score(y_test_enc, y_test_preds),\n",
    "        'Confusion Matrix' : confusion_matrix(y_test_enc, y_test_preds),\n",
    "        'Best Params': gsearch.best_estimator_\n",
    "    }\n",
    "\n",
    "tune_results_df = pd.DataFrame(tune_results).T.drop(columns=['Confusion Matrix']).sort_values(by='Test Accuracy', ascending=False)\n",
    "tune_results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings - Conclusion\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "After Hyperparameter Tuning, Decision Tree performs significantly better although marginally less Accurate the SVM. But given the time take is less than 1 sec, it would be the model of choice.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "import math\n",
    "\n",
    "# add Logistic Regression results to the results\n",
    "tune_results['Logistic Regression'] = {\n",
    "    'Train Time': results['LogisticRegression']['Time Taken'],\n",
    "    'Train Accuracy': results['LogisticRegression']['Accuracy Score'],\n",
    "    'Confusion Matrix' : results['LogisticRegression']['Confusion Matrix'],\n",
    "    'Best Params': results['LogisticRegression']['Best Params'],\n",
    "}\n",
    "\n",
    "# calc the # of rows and columns\n",
    "n_models = len(tune_results)\n",
    "n_cols = 2 # default for now\n",
    "n_rows = math.ceil(n_models / n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols = n_cols, figsize=(n_cols * 3, n_rows*3))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# One way to plot it\n",
    "for idx, (model_name, metrics) in enumerate(tune_results.items()):\n",
    "    conf_matrix = metrics['Confusion Matrix']\n",
    "    disp = ConfusionMatrixDisplay(conf_matrix)\n",
    "    disp.plot(ax=axes[idx])\n",
    "    axes[idx].set_title(model_name)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How would the model perform if other hyperparam tuning Estimators(other than GridSearchCV) such as RandomSearch or HalvingGridSearchCV or HalvingRandomSearchCV were used ?\n",
    "2. Can we run PCA as part of Feature Engineering ? What would the outcome look like ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-ai-class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
