{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce049c65",
   "metadata": {},
   "source": [
    "<font size=\"4\" color=\"red\">NOTE : This is a Work-In-Progress in refactoring the code to make it easier to run multiple models against the same dataset that has gone through Data Analysis and Preparation for modeling.</font>\n",
    "This specific file will run the classic models RandomForest and XGBoost on the datasets and predict the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783fff44",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "### Using classic ML models \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b20483",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4501bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import joblib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cc12e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot Confusion Matrix\n",
    "def plot_confusion_matrix(conf_matrix:ndarray, labels:ndarray, title=\"Confusion Matrix\"):\n",
    "\n",
    "    conf_matrix = conf_matrix[::-1]\n",
    "    # labels = labels[::-1]\n",
    "\n",
    "    #heat map\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=conf_matrix,\n",
    "        x=labels,\n",
    "        y=labels[::-1], #reverse the order to align labels with way Conf matrix is output\n",
    "        colorscale='Rainbow', # 'Hot', # 'YlOrRd', # 'YlGnBu', #'Viridis',\n",
    "        texttemplate=\"%{z}\",\n",
    "        textfont={\"size\": 10}\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text = title,\n",
    "        xaxis_title=\"Predicted Class\",\n",
    "        yaxis_title=\"Actual Class\",\n",
    "        # xaxis={'side': 'top'},\n",
    "        # yaxis={'autorange': 'reversed'},\n",
    "        width=500,\n",
    "        height=500,\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8265fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Using plotly's graph_objects\n",
    "def plot_feature_importance_comparison_plotly(pca_model: PCA, classifier_model, feature_importances: Series, X_pre_pca_df: DataFrame):\n",
    "\n",
    "    # top_features = feature_importances.head(10)\n",
    "\n",
    "    # fig = px.bar(features_df, x=features_df.index, y=features_df[0])\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=feature_importances.index,\n",
    "        y=feature_importances.values,\n",
    "        marker=dict(color='indianred'),\n",
    "        marker_color='indianred'\n",
    "    ))\n",
    "    fig.update_layout(title=f'Gene expression contribution to the model\\n{classifier_model}', template='plotly_white')\n",
    "    fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e2ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pca_features_weights(pca_model: PCA, classifier_model, feature_names: Index) :\n",
    "    '''\n",
    "    Identify the overall weights of each feature in terms of its contribution to the given model. From the given Classifier model\n",
    "    it retrieves feature importances and this data is merged with the PCA components matrix.\n",
    "    @param pca_model : PCA object, after it has been fit / trained on the data\n",
    "    @param classifier_model : Model used for classification\n",
    "    @param feature_names : List of all features used, before PCA was run.\n",
    "    @return Series consisting of contribution of each feature to the model\n",
    "    '''\n",
    "\n",
    "    # get feature contributions for each Principal Component\n",
    "    # feature_names = X_pre_pca_df.columns\n",
    "    n_components = len(pca_model.components_)\n",
    "    pca_components_df = pd.DataFrame(pca_model.components_.T,\n",
    "                                     columns=[f'PC{i+1}' for i in range(n_components)],\n",
    "                                     index=feature_names)\n",
    "\n",
    "\n",
    "    # Random forest importances for each component\n",
    "    if (hasattr(classifier_model, 'feature_importances_')):\n",
    "        rf_importances = pd.Series(\n",
    "            classifier_model.feature_importances_,\n",
    "            index=[f'PC{i+1}' for i in range(n_components)]\n",
    "        )\n",
    "\n",
    "        # --- calculate original feature importance by weighted combination ---\n",
    "        orig_importances = pca_components_df.dot(rf_importances).abs() # we only care abt the magnitude\n",
    "        # sum of all shud be 1, hence find each value's contrib to 100%\n",
    "        orig_importances = orig_importances / orig_importances.sum()\n",
    "        return orig_importances.sort_values(ascending=False)\n",
    "\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18783e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tree_path(pca_model: PCA, classifier_model, X_pca_df: DataFrame, y_series:Series, feature_names: Index, class_names=class_unique_vals):\n",
    "    '''\n",
    "    For each class / target, analyze the contribution of each feature by traversing the tree, finding the leaf node and\n",
    "    then merging that with each feature's individual weights, calculated using the function #get_pca_features_weights.\n",
    "    @param pca_model : PCA model, after the data has been fit aka trained on the data.\n",
    "    @param classifier_model : Model used for classification, after it has been trained on the data\n",
    "    @param X_pca_df : The DataFrame after PCA has been run and data has been transformed.\n",
    "    @param y_series : The Y series corresponding to above X_pca_df dataset i.e if above is `test` dataset, this should also be test dataset\n",
    "    @param feature_names : All the features used prior to running PCA\n",
    "    @param class_names : List of unique Classes / Target Variables, the data represents. In this case we have the 5 tumors.\n",
    "    @return DataFrame with columns as the 4 target classes and rows as features aka gene expressions.\n",
    "    '''\n",
    "    # feature_names = X_pca_df.columns\n",
    "    #init dictionary that will hold each class details\n",
    "    class_importances = {class_name: np.zeros(len(feature_names)) for class_name in class_unique_vals}\n",
    "\n",
    "    #loop through each DecisionTree used by the model\n",
    "    for tree in classifier_model.estimators_:\n",
    "        tree_importances = tree.feature_importances_\n",
    "        # get index of leaf node where sample is predicted\n",
    "        leaf_nodes = tree.apply(X_pca_df)\n",
    "\n",
    "        for row_idx, leaf_id in enumerate(leaf_nodes):\n",
    "            # leaf_nodes has only the node number. Not the index that matches against corresponding index in y_series. Hence we use X_pca_df to get the index\n",
    "            #row_idx is sequential increment of leaf_nodes\n",
    "            record_index = X_pca_df.index[row_idx]\n",
    "            predicted_class = y_series[record_index]\n",
    "\n",
    "            original_importances = get_pca_features_weights(pca_model, classifier_model, feature_names)\n",
    "            class_importances[predicted_class] += original_importances\n",
    "\n",
    "\n",
    "    #Normalize\n",
    "    for class_name in class_names:\n",
    "        total = np.sum(class_importances[class_name])\n",
    "        if total > 0:\n",
    "            class_importances[class_name] /= total\n",
    "\n",
    "    return pd.DataFrame(class_importances)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a7912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SHAP to explain - #TODO\n",
    "import shap\n",
    "\n",
    "def plot_SHAP(pca_model:PCA, classifier_model, X_df: DataFrame):\n",
    "    explainer = shap.Explainer(classifier_model)\n",
    "    shap_values = explainer(X_df) #X_pca_dataframe\n",
    "    original_shap = shap_values.values @ pca_model.components_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7987954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_binary(y_series: Series):\n",
    "    label_binzer = LabelBinarizer()\n",
    "    label_binzer.fit(y_series)\n",
    "    y_series_bin = np.array(label_binzer.transform(y_series))\n",
    "    return y_series_bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5304b4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(model):\n",
    "\n",
    "    pca_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', pca),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    return pca_pipeline\n",
    "\n",
    "\n",
    "def create_grid_model(model, model_params: dict):\n",
    "    # call create Pipeline\n",
    "    pipeline = create_pipeline(model)\n",
    "    # cross validation param is default = 5. n_jobs configured as param\n",
    "    grid_search = GridSearchCV(estimator=pipeline, param_grid=model_params, scoring='accuracy', n_jobs=1, refit=True, verbose=1)\n",
    "\n",
    "    return grid_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762b9deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_config = {\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'model__n_estimators':[50, 100, 200],\n",
    "            'model__max_depth': [None, 10, 20],\n",
    "            'model__min_samples_split': [2, 5]\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),\n",
    "        'params': {\n",
    "            'model__n_estimators':[50, 100, 200],\n",
    "            'model__max_depth': [None, 10, 20],\n",
    "            'model__eta': [0.2, None, 0.4],\n",
    "            #'model__n_jobs': [1]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a283e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_results = {}\n",
    "def fit_model(model, model_params:dict, X_df: DataFrame, y_series:Series):\n",
    "    '''\n",
    "    @param model : Classifier model\n",
    "    @param model_params : Params used for tuning the hyperparameters of the given model\n",
    "    @param X_df : DataFrame of the independent vars with data\n",
    "    @param y_series : A Series object with target class data.\n",
    "    @return dict : Consisting of keys : Execution Time, Accuracy, Precision, Recall, F1-Score, Confusion matrix, Best Estimate, Best Params, Best Accuracy\n",
    "    '''\n",
    "    # for model_name, model_params in models_config.items():\n",
    "    start_time = time()\n",
    "\n",
    "    grid_search = create_grid_model(model=model, model_params=model_params)\n",
    "    grid_search.fit(X_df, y_series)\n",
    "\n",
    "    end_time = time()\n",
    "\n",
    "    return grid_search\n",
    "\n",
    "\n",
    "\n",
    "def predict_model(grid_search: GridSearchCV, X_df: DataFrame, y_series: Series):\n",
    "    start_time = time()\n",
    "\n",
    "    y_preds = grid_search.predict(X_df)\n",
    "\n",
    "    end_time = time()\n",
    "\n",
    "    best_results = {\n",
    "        'Execution Time': (end_time - start_time),\n",
    "        'Accuracy': accuracy_score(y_series, y_preds),\n",
    "        'Precision': precision_score(y_series, y_preds, average='weighted'),\n",
    "        'Recall': recall_score(y_series, y_preds,  average='weighted'),\n",
    "        'F1-Score': f1_score(y_series, y_preds, average='weighted'),\n",
    "        'Confusion matrix': confusion_matrix(y_series, y_preds),\n",
    "        # 'GridSearchCV': grid_search,\n",
    "        'Best Estimate': grid_search.best_estimator_,\n",
    "        'Best Params': grid_search.best_params_,\n",
    "        'Best Accuracy': grid_search.best_score_\n",
    "    }\n",
    "\n",
    "    return best_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273197e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# best_results = fit_predict_model(X_train_pca, y_train)\n",
    "\n",
    "# Random Forest :\n",
    "rf_model = models_config['Random Forest']['model']\n",
    "rf_model_config = models_config['Random Forest']['params']\n",
    "rf_grid_search = fit_model(model=rf_model, model_params=rf_model_config, X_df=X_train_pca, y_series=y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc13f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_X_test_pca = clean_input_data(X_test, df_feature_outliers)\n",
    "\n",
    "rf_best_results = predict_model(grid_search=rf_grid_search, X_df=rf_X_test_pca, y_series=y_test)\n",
    "print(rf_best_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71035cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_conf_matrix = rf_best_results['Confusion matrix']\n",
    "plot_confusion_matrix(rf_conf_matrix, labels=class_unique_vals, title='Random Forest with PCA : Confusion Matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c34079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimator :\n",
    "rf_pca_model = rf_grid_search.best_estimator_['pca']\n",
    "rf_trained_model = rf_grid_search.best_estimator_['model']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcae8312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find contribution of each feature to the model\n",
    "rf_pca_weights_df = get_pca_features_weights(pca_model=rf_pca_model,\n",
    "                                            classifier_model=rf_trained_model,\n",
    "                                            feature_names=rf_X_test_pca.columns)\n",
    "if rf_pca_weights_df is not None:\n",
    "    # plot_feature_importance_comparison(pca_model, rd_model, pca_weights_df, X_pca_dataframe)\n",
    "    plot_feature_importance_comparison_plotly(rf_pca_model, rf_model, rf_pca_weights_df, rf_X_test_pca)\n",
    "else :\n",
    "    print(f'PCA Weights for {rf_trained_model} could not be calculated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efa761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_pca_weights_df[rf_pca_weights_df > 0][1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f83488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/RForest_model.pkl']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "joblib.dump(rf_trained_model, f\"{MODELS_DIR}RForest_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07db4cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test to make sure persisted model can be reloaded correctly.\n",
    "rf_model_joblib: RandomForestClassifier = joblib.load(f\"{MODELS_DIR}RForest_model.pkl\")\n",
    "print(rf_model_joblib.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d2227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHAP :\n",
    "# plot_SHAP(pca_model=rf_pca_model, classifier_model=rf_trained_model, X_df=rf_X_test_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4080d9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB requires target vars in binary format and hence cannot reuse the fn used for RandomForest\n",
    "def fit_xgb_classifier(model: XGBClassifier, model_params:dict, X_df: DataFrame, y_series:ndarray):\n",
    "    '''\n",
    "    @param model : Classifier model\n",
    "    @param model_params : Params used for tuning the hyperparameters of the given model\n",
    "    @param X_df : DataFrame of the independent vars with data\n",
    "    @param y_series : An ndarray object with binary representation of target classes\n",
    "    @return dict : Consisting of keys : Execution Time, Accuracy, Precision, Recall, F1-Score, Confusion matrix, Best Estimate, Best Params, Best Accuracy\n",
    "    '''\n",
    "    # for model_name, model_params in models_config.items():\n",
    "    start_time = time()\n",
    "\n",
    "    grid_search = create_grid_model(model=model, model_params=model_params)\n",
    "    grid_search.fit(X_df, y_series)\n",
    "\n",
    "    end_time = time()\n",
    "\n",
    "    return grid_search\n",
    "\n",
    "\n",
    "def predict_xgb_classifier(grid_search: GridSearchCV, X_df: DataFrame, y_series_bin: ndarray):\n",
    "    start_time = time()\n",
    "\n",
    "    y_preds = grid_search.predict(X_df)\n",
    "\n",
    "    end_time = time()\n",
    "\n",
    "    y_test_bin_arr = np.array(y_series_bin)\n",
    "    # confusion matrix\n",
    "    cm = confusion_matrix(y_true=np.argmax(y_test_bin_arr, axis=1), y_pred=np.argmax(y_preds, axis=1)) #, labels=class_unique_vals)\n",
    "    # plot_confusion_matrix(cm, labels=class_unique_vals, title='XGBoost with PCA : Confusion Matrix')\n",
    "\n",
    "    best_results = {\n",
    "        'Execution Time': (end_time - start_time),\n",
    "        'Accuracy': accuracy_score(y_series_bin, y_preds),\n",
    "        'Precision': precision_score(y_series_bin, y_preds, average='weighted'),\n",
    "        'Recall': recall_score(y_series_bin, y_preds,  average='weighted'),\n",
    "        'F1-Score': f1_score(y_series_bin, y_preds, average='weighted'),\n",
    "        'Confusion matrix': cm,\n",
    "        # 'GridSearchCV': grid_search,\n",
    "        'Best Estimate': grid_search.best_estimator_,\n",
    "        'Best Params': grid_search.best_params_,\n",
    "        'Best Accuracy': grid_search.best_score_\n",
    "    }\n",
    "\n",
    "    return best_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6f5c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    }
   ],
   "source": [
    "xgb_model = models_config['XGBoost']['model']\n",
    "xgb_model_config = models_config['XGBoost']['params']\n",
    "\n",
    "y_train_bin = convert_to_binary(y_train)\n",
    "# print(y_train_bin)\n",
    "# Uncomment below to identify mappings between binary format and the actual label\n",
    "# 0=BRCA 1=COAD, 2=KIRC, 3=LUAD, 4=PRAD\n",
    "# print(\"Binary data: \", y_train_bin)\n",
    "# print(\"Label to int mapping: \", label_binzer.inverse_transform(np.array(y_train_bin)))\n",
    "\n",
    "xgb_grid_search = fit_xgb_classifier(model=xgb_model, model_params=xgb_model_config, X_df=X_train_pca, y_series=y_train_bin)\n",
    "\n",
    "# xgb_train_best_results = predict_xgb_classifier(grid_search=xgb_grid_search, X_df=X_train_pca, y_series_bin=y_train_bin)\n",
    "# print(xgb_train_best_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0745e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_y_test_bin = convert_to_binary(y_test)\n",
    "xgb_X_test_pca = clean_input_data(X_test, df_feature_outliers)\n",
    "\n",
    "xgb_best_results = predict_xgb_classifier(grid_search=xgb_grid_search, X_df=xgb_X_test_pca, y_series_bin=xgb_y_test_bin)\n",
    "print(xgb_best_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37477203",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_conf_matrix = xgb_best_results['Confusion matrix']\n",
    "plot_confusion_matrix(xgb_conf_matrix, labels=class_unique_vals, title='XGBoost with PCA : Confusion Matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0276ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pca_model = xgb_grid_search.best_estimator_['pca']\n",
    "xgb_trained_model = xgb_grid_search.best_estimator_['model']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7226ae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find contribution of each feature to the model\n",
    "xgb_pca_weights_df = get_pca_features_weights(pca_model=xgb_pca_model,\n",
    "                                            classifier_model=xgb_trained_model,\n",
    "                                            feature_names=xgb_X_test_pca.columns)\n",
    "if xgb_pca_weights_df is not None:\n",
    "    # plot_feature_importance_comparison(pca_model, rd_model, pca_weights_df, X_pca_dataframe)\n",
    "    plot_feature_importance_comparison_plotly(xgb_pca_model, xgb_trained_model, xgb_pca_weights_df, xgb_X_test_pca)\n",
    "else :\n",
    "    print(f'PCA Weights for {rf_trained_model} could not be calculated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dda88e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persisting data for tests\n",
    "# print(xgb_X_test_pca.head())\n",
    "xgb_X_test_pca.to_csv(DATA_DIR + 'xgb_X_after_pca_dataset.csv')\n",
    "print('516: ', y_test[516])\n",
    "print('329: ', y_test[329])\n",
    "print('52: ', y_test[52])\n",
    "print('141: ', y_test[141])\n",
    "\n",
    "\n",
    "\n",
    "# Generate statistical description of these columns to help generate random values on the App side - deprecated\n",
    "len(xgb_X_test_pca.columns)\n",
    "xgb_test_descr_df = xgb_X_test_pca.describe().T\n",
    "# xgb_test_descr_df.to_csv(DATA_DIR + 'xgb_test_X_describe.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aeed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pca_model.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e48ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/xgb_GridSearch_Pipeline.pkl']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Persist just the XGBooster model. If used the inputs would be 640 PCA components which can be identified using xgb_pca_model.get_feature_names_out()\n",
    "joblib.dump(xgb_trained_model, f\"{MODELS_DIR}XGBoost_model.pkl\")\n",
    "\n",
    "# persis the whole trained GridSearchCV\n",
    "joblib.dump(xgb_grid_search, f\"{MODELS_DIR}xgb_GridSearch_Pipeline.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e131e5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test to make sure persisted model can be reloaded correctly.\n",
    "# xgb_model_joblib: XGBClassifier = joblib.load(f\"{MODELS_DIR}XGBoost_model.pkl\")\n",
    "# print(xgb_model_joblib.feature_importances_)\n",
    "\n",
    "xgb_gridcv_pipeline_joblib: GridSearchCV = joblib.load(f\"{MODELS_DIR}xgb_GridSearch_Pipeline.pkl\")\n",
    "xgb_gridcv_pipeline_joblib.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cbcea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit testing : to make sure model prediction is same as when input from Web App\n",
    "unittest_data_X = pd.read_csv(DATA_ANALYSIS_DIR + 'request.csv')\n",
    "# print(\"Printing req data: \\n\", type(unittest_data_X.iloc[0:1, 1:]))\n",
    "unittest_data_pred = xgb_grid_search.predict(unittest_data_X.iloc[0:1, 1:])\n",
    "\n",
    "# unittest_data_pred = xgb_grid_search.predict(xgb_X_test_pca.iloc[0:1, 0:])\n",
    "unittest_data_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c50c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame([rf_best_results, xgb_best_results]).T #.sort_values(by='Accuracy', ascending=False)\n",
    "results_df.columns = ['Random Forest', 'XGBoost']\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc401d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area under Curve - REF : https://www.geeksforgeeks.org/interpreting-random-forest-classification-results/\n",
    "# -------- #TODO : need to fix the roc_curve function\n",
    "def plot_aoc_randomforest(rd_test_pred_proba: ndarray, y_test: Series) :\n",
    "    target_vals = y_test.unique()\n",
    "    # y_test_bin = label_binarize(y_test, classes=[0,1,2,3,4])\n",
    "    label_binzer = LabelBinarizer()\n",
    "    label_binzer.fit(y_test)\n",
    "    y_test_bin = np.array(label_binzer.transform(y_test))\n",
    "    print(y_test_bin[0])\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    print(f\"y_test_bin : {y_test_bin[1]}\")\n",
    "    print(f\"rd_preds_prob : {rd_test_pred_proba[:, 1]}\")\n",
    "    # print(f\"y_test_bin : {len(y_test_bin[:, 1])}\")\n",
    "    # print(f\"rd_preds_prob : {rd_preds_prob[:, 1]}\")\n",
    "    # print(rd_preds_prob)\n",
    "\n",
    "    for index in range(len(target_vals)):\n",
    "        fpr[index], tpr[index], _ = roc_curve(y_test_bin[index], rd_test_pred_proba[:, index])\n",
    "        # print(f\"FPR at {index}: \\n{fpr[index]}\")\n",
    "        # print(f\"TPR at {index}: \\n{tpr[index]}\")\n",
    "        roc_auc[index] = auc(fpr[index], tpr[index])\n",
    "\n",
    "    # Plot ROC curve\n",
    "    # plt.figure()\n",
    "    # for index in range(len(target_vals)) :\n",
    "    #     plt.plot(fpr[index], tpr[index], lw=2, label=f\"ROC curve of class {target_vals[index]} (area = {roc_auc[index]:.2f})\")\n",
    "\n",
    "    # # plt.plot([0,1], [0,1], color='navy', lw=2, linestyle='--')\n",
    "    # plt.xlim([0.0, 1.0])\n",
    "    # plt.ylim([0.0, 1.05])\n",
    "    # plt.xlabel('False Positive Rate')\n",
    "    # plt.ylabel('True Positive Rate')\n",
    "    # plt.title('Receiver Operating Characterstic for Tumor classes')\n",
    "    # plt.legend(loc=\"lower right\")\n",
    "    # plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
