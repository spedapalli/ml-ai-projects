{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gene Expressions for different types of tumor\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/gene_protein_cancer.jpg\" alt=\"image\" width=\"300\" height=\"200\">\n",
    "</div>\n",
    "[image src: https://www.cancer.gov/about-cancer/causes-prevention/genetics]\n",
    "\n",
    "This project aims to identify different gene expressions associated to 5 types of tumor : \n",
    "- BRCA (Breast Cancer): Family of Genes (BRCA1 and BRCA2) are known as tumor suppresors. But mutation in these genes cause cancer.\n",
    "- KIRC (Kidney Renal Clear Cell Carcinoma): \n",
    "- COAD (Colon Adenocarcinoma)\n",
    "- LUAD (Lung Adenocarcinoma)\n",
    "- PRAD (Prostate Adenocarcinoma)\n",
    "\n",
    "The dataset is sourced from https://archive.ics.uci.edu/dataset/401/gene+expression+cancer+rna+seq.\n",
    "The original dataset is published at https://www.synapse.org/Synapse:syn300013/discussion/threadId=5455. The Gene names in the dataset are dummy names. The actual gene names are at https://www.ncbi.nlm.nih.gov/gene, per this discussion thread https://www.synapse.org/Synapse:syn300013/discussion/threadId=5455. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATA_ANALYSIS_DIR = \"data-analysis/\"\n",
    "MODELS_DIR = \"models/\"\n",
    "DATA_DIR = \"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "from numpy import ndarray\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas.core.indexes.base import Index\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.utils.ArrayUtils import get_list_of_items_in_both_lists\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from files\n",
    "url_reconstructed = 'TCGA-PANCAN-HiSeq-801x20531/data.csv'\n",
    "\n",
    "# url = 'https://drive.google.com/file/d/1VXyhDXpYT8G2Buhkc6kBjw93CLG1y1f0/view?usp=drive_link'\n",
    "# # Use only the Id and reconstruct the URL\n",
    "# url_reconstructed = 'https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
    "\n",
    "df = pd.read_csv(url_reconstructed)\n",
    "tumor_df = pd.read_csv('TCGA-PANCAN-HiSeq-801x20531/labels.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataframe before adding the class column: {df.info()}\")\n",
    "print(f\"Total # of columns: {len(df.columns)}\")\n",
    "print(\"COlumns type\", type(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy over the Class column into the main dataframe\n",
    "df = df.rename(columns={'Unnamed: 0': 'sample'})\n",
    "tumor_df = tumor_df.rename(columns={'Unnamed: 0': 'sample'})\n",
    "df['Class'] = np.where( (df['sample'] == tumor_df['sample']), tumor_df['Class'], df['sample'])\n",
    "\n",
    "# df['Class'] = tumor_df['Class']\n",
    "# print(f\"Total # of columns: {len(df.columns)}\")\n",
    "\n",
    "df.drop(columns=['sample'], axis=1, inplace=True)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "### Data Cleaning and PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data : \n",
    "- No NANs as stated on the data source page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns with 0 values\n",
    "zero_cols = df.columns[(df == 0).all()]\n",
    "print(f\"# of columns with all 0s in them : {zero_cols}\")\n",
    "df = df.drop(columns=zero_cols, axis=1)\n",
    "print(f\"# of columns with all 0s in them : {df.columns[(df == 0).all()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post cleanup, write the data to file\n",
    "df.to_csv(DATA_ANALYSIS_DIR + \"df_PostColumnCleanup.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class analysis\n",
    "class_unique_vals = df['Class'].unique()\n",
    "assert(len(class_unique_vals) == 5)\n",
    "\n",
    "# print the distribution\n",
    "print(f\"Distribution of Class values: \\n{df['Class'].value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split -\n",
    "X = df.drop(columns=['Class'], axis=1)\n",
    "y = df['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=df['Class'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary : Data Cleaning and PreProcessing : \n",
    "- 0 NaNs\n",
    "- 267 columns with only 0s as values. Dropped all of these columns\n",
    "- 5 target values aka classifications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate and Multivariate Analysis\n",
    "Given the large # of variables (columns), charting each out against other can be a challenge. Hence printing key insights such as stats to file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Statistical Description of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "df_desc = df.describe()\n",
    "df_desc_t = df_desc.T\n",
    "# NOTE: We use this data to generate random test data when testing from App.\n",
    "df_desc_t.to_csv(DATA_ANALYSIS_DIR + \"df_describe.csv\")\n",
    "\n",
    "df_desc_zscore = df_desc.apply(stats.zscore)\n",
    "df_desc_zscore.T.to_csv(DATA_ANALYSIS_DIR + \"rawdata_zscore.csv\")\n",
    "# df_desc['upperbound'] = df_desc['mean'] + 3*(df_desc['std'])\n",
    "# df_desc['lowerbound'] = df_desc['mean'] - 3*(df_desc['std'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Correlation\n",
    "Commented out since it takes too long to execute. Not much insight could be obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NOTE : This take quite some time to run 11+ mins\n",
    "# corr_df = X.corr()\n",
    "# corr_df.to_csv(DATA_ANALYSIS_DIR + 'features_corr_matrix.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there any Features with Correlation value= NaN ?\n",
    "# -- In total there are 267 cols with 0 values, since they are deleted, result is zero\n",
    "# corr_nan_list = corr_df.columns[corr_df.isna().all()].tolist()\n",
    "# print(corr_nan_list)\n",
    "# corr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Statistical Description per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gene values (mean, mode, min, max, range) per class\n",
    "# ---- NOTE : Below can take about 30secs to run.\n",
    "# (X['gene_0'] == 0).sum()\n",
    "\n",
    "def describeEachClass():\n",
    "    for i in range(len(class_unique_vals)) :\n",
    "        cls = class_unique_vals[i]\n",
    "        # print('i= ', i, 'class= ', cls)\n",
    "        df[df['Class'] == cls].describe().T.to_csv(f'{DATA_ANALYSIS_DIR}{cls}_describe_output.csv')\n",
    "\n",
    "describeEachClass()\n",
    "# print(type(df[df['Class'] == 'BRCA'].describe()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data has columns where values are close to 100% quartile. Get a list of these columns\n",
    "\n",
    "def get_features_with_4thquartile_values_only(df) :\n",
    "    '''\n",
    "    @param df : Dataframe with all the data\n",
    "    @return Dataframe consisting of descriptive stats for features in  in `df` where all values fall in 4th quartile.\n",
    "    '''\n",
    "    #generate descriptive statistics of given dataframe\n",
    "    df_desc = df.describe()\n",
    "    df_desc_t = df_desc.T\n",
    "\n",
    "    print(f\"Total # of columns: {len(df_desc_t)}\")\n",
    "    df_cols_outliers = df_desc_t[(df_desc_t['25%'] == 0) & (df_desc_t['50%'] == 0) & (df_desc_t['75%'] == 0)]\n",
    "    print(f\"# of columns with only outliers: {len(df_cols_outliers)}\")\n",
    "\n",
    "    return df_cols_outliers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify if any of above columns have patterns such as associated with a type of tumor.\n",
    "# Output the count of rows per Tumor type, for each of the above genes\n",
    "def get_features_target_class_counts(df, df_cols_outliers, target_col='Class'):\n",
    "    '''\n",
    "    @param df : Dataframe with all the data\n",
    "    @param df_cols_outliers : Dataframe with descriptive stats of features. See #get_features_with_4thquartile_values_only(df)\n",
    "    @param target_col : Dependent variable in dataframe `df`.\n",
    "    '''\n",
    "    df_col_out_values = pd.DataFrame()\n",
    "    for i in range(len(df_cols_outliers)):\n",
    "        # for each record in outliers df, get only the rows that hv value greater than 0\n",
    "        df_outliers_temp = df[df[df_cols_outliers.index[i]] > 0][[target_col, df_cols_outliers.index[i]]]\n",
    "        df_outliers_temp1 = df_outliers_temp[target_col].value_counts().to_frame().T\n",
    "        df_outliers_temp1['gene'] = df_cols_outliers.index[i]\n",
    "\n",
    "        df_col_out_values = pd.concat([df_col_out_values, df_outliers_temp1], ignore_index=True)\n",
    "\n",
    "        # df_col_out_values = pd.concat([df_col_out_values, df_outliers_temp], ignore_index=True)\n",
    "\n",
    "    return df_col_out_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Features with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_outliers = get_features_with_4thquartile_values_only(df)\n",
    "df_feature_outliers_count = get_features_target_class_counts(df, df_feature_outliers, 'Class')\n",
    "df_feature_outliers_count.to_csv(f'{DATA_ANALYSIS_DIR}Outliers_cols_values_count.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import math\n",
    "\n",
    "# init vars to help draw the layout\n",
    "max_rows = 8\n",
    "max_cols = 4\n",
    "layout_diviser = min(max_rows, max_cols)\n",
    "\n",
    "# vars to help place graph in the matrix\n",
    "print(len(df_feature_outliers_count))\n",
    "# no of features to plot per graph determined based on graph matrix size\n",
    "features_per_graph = math.ceil( len(df_feature_outliers_count) / (max_rows * max_cols) )\n",
    "start_index: int = 0\n",
    "end_index: int = features_per_graph\n",
    "\n",
    "# Total # of graphs to plot\n",
    "total_graphs = math.ceil(len(df_feature_outliers_count) / features_per_graph)\n",
    "print(f\"# of feature groups: {total_graphs}\")\n",
    "\n",
    "# init plot\n",
    "fig = make_subplots(rows=max_rows, cols=max_cols, shared_xaxes=True, vertical_spacing=0.03)\n",
    "\n",
    "col_headers = df_feature_outliers_count.columns.to_list()\n",
    "\n",
    "# Loop through # of graphs to plot. For each graph, plot the determined # of features\n",
    "for fg in range(total_graphs):\n",
    "    row = (fg) // layout_diviser\n",
    "    col = (fg) % layout_diviser\n",
    "    # print(f\"row= {row} col= {col}\")\n",
    "    plot_df = df_feature_outliers_count.iloc[start_index:end_index, :]\n",
    "    for i in range(len(plot_df)):\n",
    "        fig.add_trace(go.Scatter(x=col_headers,\n",
    "                                 y=plot_df.iloc[i, :-1],\n",
    "                                 mode='lines',\n",
    "                                 name=plot_df.loc[plot_df.index[i], 'gene']),\n",
    "                                 row=row+1, col=col+1) #add_trace row and col, start index is 1, while row and col start with 0\n",
    "\n",
    "    start_index = start_index + features_per_graph\n",
    "    end_index = end_index + features_per_graph\n",
    "\n",
    "\n",
    "fig.update_layout(height=1200, width=800, title_text=\"Outlier columns only : Records with (non-zero) values across the 5 classes\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary : Univariate and Multivariate Analysis\n",
    "\n",
    "- Descriptive Statistics of data : 1660 columns have mostly 0s, as values and their values fall into the 4th quartile.\n",
    "- Outliers : Given the above 1660 columns / features have largely outliders, from above graph we identify the count of actual rows having these outlier values is less than 150, mostly less than 100. \n",
    "- Correlation : Given the large number of features, finding correlation between the features is challenging. The output file does not even open. Thus no inference to make.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA\n",
    "To meet PCA needs, we need to ensure data confirms to below : \n",
    "- <u>correlation</u> between Independent vars.\n",
    "- Variables must be <u>continuous</u> i.e numeric\n",
    "- Variables must <u>be on same scale</u>. If one is in 1000s and another single digit, we will need to scale it.\n",
    "- Data must be free from outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get quartiles for each column\n",
    "def get_quartiles(col):\n",
    "    quartile1, quartile3 = col.quantile([0.25, 0.75])\n",
    "    inter_quartile_range = quartile3 - quartile1\n",
    "    lower_range = quartile1 - (1.5* inter_quartile_range)\n",
    "    upper_range = quartile3 + (1.5* inter_quartile_range)\n",
    "\n",
    "    return lower_range, upper_range\n",
    "\n",
    "# lowerQ, upperQ = get_quartiles(X_train['gene_1'])\n",
    "# print(f'lower quantile= {lowerQ}, upper quantile= {upperQ}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = X_train_pca.columns\n",
    "\n",
    "def normalize_within_quartiles(df_X:DataFrame, cols:Index):\n",
    "    '''\n",
    "    @input df_X : Dataframe with ONLY independent variables.\n",
    "    '''\n",
    "    for i in cols :\n",
    "        LL, UL = get_quartiles(df_X[i])\n",
    "        # if (i == 'gene_0'):\n",
    "        #     print(f\"UL: {np.where(df_X[i] > UL, UL, df_X[i])}\")\n",
    "            # print(f\"{np.where(df_X[i] < LL, LL, df_X[i]}\")\n",
    "        df_X[i] = np.where(df_X[i] > UL, UL, df_X[i])\n",
    "        df_X[i] = np.where(df_X[i] < LL, LL, df_X[i])\n",
    "    return df_X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(df_feature_outliers.index.to_list()))\n",
    "def clean_input_data(X_dataframe: DataFrame, df_feature_outliers: DataFrame):\n",
    "    # print(len(X_train.columns))\n",
    "    X_train_pca = X_dataframe.drop(columns=df_feature_outliers.index.to_list())\n",
    "    # X_train_pca.columns\n",
    "    X_train_pca = normalize_within_quartiles(X_train_pca, X_train_pca.columns)\n",
    "    X_train_pca.head(10)\n",
    "    print(f\"Total # of cols after removing quartiles: {len(X_train_pca.columns)}\")\n",
    "\n",
    "    return X_train_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prep for PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA -\n",
    "# -- Is PCA the right approach to this data set where we are trying to identify Gene expressions for tumors.\n",
    "# -- Shudnt we include every expression - small or big ?\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove Outliers\n",
    "Based on data analysis on [outliers](#features-with-outliers) we know there are 1660 features with values in the 4th quartile. These gene expressions (features) are not specific to a tumor type (Class aka dependent variable) as observed in the output of function #get_features_target_class_counts. We will assume that these Gene expressions have not been recorded accurately or these have minimal influence from a tumor. Hence we remove these before running PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = clean_input_data(X_train, df_feature_outliers)\n",
    "X_train_pca.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary : Feature Selection\n",
    "- Features with outliers values (in 4th quartile) dropped\n",
    "- PCA reduces features to 640, from 18604 features. Data prepared for PCA execution in next section, as part of the Pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import joblib\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utility Functions for Graphing and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot Confusion Matrix\n",
    "def plot_confusion_matrix(conf_matrix:ndarray, labels:ndarray, title=\"Confusion Matrix\"):\n",
    "\n",
    "    conf_matrix = conf_matrix[::-1]\n",
    "    # labels = labels[::-1]\n",
    "\n",
    "    #heat map\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=conf_matrix,\n",
    "        x=labels,\n",
    "        y=labels[::-1], #reverse the order to align labels with way Conf matrix is output\n",
    "        colorscale='Rainbow', # 'Hot', # 'YlOrRd', # 'YlGnBu', #'Viridis',\n",
    "        texttemplate=\"%{z}\",\n",
    "        textfont={\"size\": 10}\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text = title,\n",
    "        xaxis_title=\"Predicted Class\",\n",
    "        yaxis_title=\"Actual Class\",\n",
    "        # xaxis={'side': 'top'},\n",
    "        # yaxis={'autorange': 'reversed'},\n",
    "        width=500,\n",
    "        height=500,\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Using plotly's graph_objects\n",
    "def plot_feature_importance_comparison_plotly(pca_model: PCA, classifier_model, feature_importances: Series, X_pre_pca_df: DataFrame):\n",
    "\n",
    "    # top_features = feature_importances.head(10)\n",
    "\n",
    "    # fig = px.bar(features_df, x=features_df.index, y=features_df[0])\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=feature_importances.index,\n",
    "        y=feature_importances.values,\n",
    "        marker=dict(color='indianred'),\n",
    "        marker_color='indianred'\n",
    "    ))\n",
    "    fig.update_layout(title=f'Gene expression contribution to the model\\n{classifier_model}', template='plotly_white')\n",
    "    fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pca_features_weights(pca_model: PCA, classifier_model, feature_names: Index) :\n",
    "    '''\n",
    "    Identify the overall weights of each feature in terms of its contribution to the given model. From the given Classifier model\n",
    "    it retrieves feature importances and this data is merged with the PCA components matrix.\n",
    "    @param pca_model : PCA object, after it has been fit / trained on the data\n",
    "    @param classifier_model : Model used for classification\n",
    "    @param feature_names : List of all features used, before PCA was run.\n",
    "    @return Series consisting of contribution of each feature to the model\n",
    "    '''\n",
    "\n",
    "    # get feature contributions for each Principal Component\n",
    "    # feature_names = X_pre_pca_df.columns\n",
    "    n_components = len(pca_model.components_)\n",
    "    pca_components_df = pd.DataFrame(pca_model.components_.T,\n",
    "                                     columns=[f'PC{i+1}' for i in range(n_components)],\n",
    "                                     index=feature_names)\n",
    "\n",
    "\n",
    "    # Random forest importances for each component\n",
    "    if (hasattr(classifier_model, 'feature_importances_')):\n",
    "        rf_importances = pd.Series(\n",
    "            classifier_model.feature_importances_,\n",
    "            index=[f'PC{i+1}' for i in range(n_components)]\n",
    "        )\n",
    "\n",
    "        # --- calculate original feature importance by weighted combination ---\n",
    "        orig_importances = pca_components_df.dot(rf_importances).abs() # we only care abt the magnitude\n",
    "        # sum of all shud be 1, hence find each value's contrib to 100%\n",
    "        orig_importances = orig_importances / orig_importances.sum()\n",
    "        return orig_importances.sort_values(ascending=False)\n",
    "\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tree_path(pca_model: PCA, classifier_model, X_pca_df: DataFrame, y_series:Series, feature_names: Index, class_names=class_unique_vals):\n",
    "    '''\n",
    "    For each class / target, analyze the contribution of each feature by traversing the tree, finding the leaf node and\n",
    "    then merging that with each feature's individual weights, calculated using the function #get_pca_features_weights.\n",
    "    @param pca_model : PCA model, after the data has been fit aka trained on the data.\n",
    "    @param classifier_model : Model used for classification, after it has been trained on the data\n",
    "    @param X_pca_df : The DataFrame after PCA has been run and data has been transformed.\n",
    "    @param y_series : The Y series corresponding to above X_pca_df dataset i.e if above is `test` dataset, this should also be test dataset\n",
    "    @param feature_names : All the features used prior to running PCA\n",
    "    @param class_names : List of unique Classes / Target Variables, the data represents. In this case we have the 5 tumors.\n",
    "    @return DataFrame with columns as the 4 target classes and rows as features aka gene expressions.\n",
    "    '''\n",
    "    # feature_names = X_pca_df.columns\n",
    "    #init dictionary that will hold each class details\n",
    "    class_importances = {class_name: np.zeros(len(feature_names)) for class_name in class_unique_vals}\n",
    "\n",
    "    #loop through each DecisionTree used by the model\n",
    "    for tree in classifier_model.estimators_:\n",
    "        tree_importances = tree.feature_importances_\n",
    "        # get index of leaf node where sample is predicted\n",
    "        leaf_nodes = tree.apply(X_pca_df)\n",
    "\n",
    "        for row_idx, leaf_id in enumerate(leaf_nodes):\n",
    "            # leaf_nodes has only the node number. Not the index that matches against corresponding index in y_series. Hence we use X_pca_df to get the index\n",
    "            #row_idx is sequential increment of leaf_nodes\n",
    "            record_index = X_pca_df.index[row_idx]\n",
    "            predicted_class = y_series[record_index]\n",
    "\n",
    "            original_importances = get_pca_features_weights(pca_model, classifier_model, feature_names)\n",
    "            class_importances[predicted_class] += original_importances\n",
    "\n",
    "\n",
    "    #Normalize\n",
    "    for class_name in class_names:\n",
    "        total = np.sum(class_importances[class_name])\n",
    "        if total > 0:\n",
    "            class_importances[class_name] /= total\n",
    "\n",
    "    return pd.DataFrame(class_importances)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SHAP to explain - #TODO\n",
    "import shap\n",
    "\n",
    "def plot_SHAP(pca_model:PCA, classifier_model, X_df: DataFrame):\n",
    "    explainer = shap.Explainer(classifier_model)\n",
    "    shap_values = explainer(X_df) #X_pca_dataframe\n",
    "    original_shap = shap_values.values @ pca_model.components_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_binary(y_series: Series):\n",
    "    label_binzer = LabelBinarizer()\n",
    "    label_binzer.fit(y_series)\n",
    "    y_series_bin = np.array(label_binzer.transform(y_series))\n",
    "    return y_series_bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(model):\n",
    "\n",
    "    pca_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', pca),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    return pca_pipeline\n",
    "\n",
    "\n",
    "def create_grid_model(model, model_params: dict):\n",
    "    # call create Pipeline\n",
    "    pipeline = create_pipeline(model)\n",
    "    # cross validation param is default = 5. n_jobs configured as param\n",
    "    grid_search = GridSearchCV(estimator=pipeline, param_grid=model_params, scoring='accuracy', n_jobs=1, refit=True, verbose=1)\n",
    "\n",
    "    return grid_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_config = {\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'model__n_estimators':[50, 100, 200],\n",
    "            'model__max_depth': [None, 10, 20],\n",
    "            'model__min_samples_split': [2, 5]\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),\n",
    "        'params': {\n",
    "            'model__n_estimators':[50, 100, 200],\n",
    "            'model__max_depth': [None, 10, 20],\n",
    "            'model__eta': [0.2, None, 0.4],\n",
    "            #'model__n_jobs': [1]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_results = {}\n",
    "def fit_model(model, model_params:dict, X_df: DataFrame, y_series:Series):\n",
    "    '''\n",
    "    @param model : Classifier model\n",
    "    @param model_params : Params used for tuning the hyperparameters of the given model\n",
    "    @param X_df : DataFrame of the independent vars with data\n",
    "    @param y_series : A Series object with target class data.\n",
    "    @return dict : Consisting of keys : Execution Time, Accuracy, Precision, Recall, F1-Score, Confusion matrix, Best Estimate, Best Params, Best Accuracy\n",
    "    '''\n",
    "    # for model_name, model_params in models_config.items():\n",
    "    start_time = time()\n",
    "\n",
    "    grid_search = create_grid_model(model=model, model_params=model_params)\n",
    "    grid_search.fit(X_df, y_series)\n",
    "\n",
    "    end_time = time()\n",
    "\n",
    "    return grid_search\n",
    "\n",
    "\n",
    "\n",
    "def predict_model(grid_search: GridSearchCV, X_df: DataFrame, y_series: Series):\n",
    "    start_time = time()\n",
    "\n",
    "    y_preds = grid_search.predict(X_df)\n",
    "\n",
    "    end_time = time()\n",
    "\n",
    "    best_results = {\n",
    "        'Execution Time': (end_time - start_time),\n",
    "        'Accuracy': accuracy_score(y_series, y_preds),\n",
    "        'Precision': precision_score(y_series, y_preds, average='weighted'),\n",
    "        'Recall': recall_score(y_series, y_preds,  average='weighted'),\n",
    "        'F1-Score': f1_score(y_series, y_preds, average='weighted'),\n",
    "        'Confusion matrix': confusion_matrix(y_series, y_preds),\n",
    "        # 'GridSearchCV': grid_search,\n",
    "        'Best Estimate': grid_search.best_estimator_,\n",
    "        'Best Params': grid_search.best_params_,\n",
    "        'Best Accuracy': grid_search.best_score_\n",
    "    }\n",
    "\n",
    "    return best_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# best_results = fit_predict_model(X_train_pca, y_train)\n",
    "\n",
    "# Random Forest :\n",
    "rf_model = models_config['Random Forest']['model']\n",
    "rf_model_config = models_config['Random Forest']['params']\n",
    "rf_grid_search = fit_model(model=rf_model, model_params=rf_model_config, X_df=X_train_pca, y_series=y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_X_test_pca = clean_input_data(X_test, df_feature_outliers)\n",
    "\n",
    "rf_best_results = predict_model(grid_search=rf_grid_search, X_df=rf_X_test_pca, y_series=y_test)\n",
    "print(rf_best_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_conf_matrix = rf_best_results['Confusion matrix']\n",
    "plot_confusion_matrix(rf_conf_matrix, labels=class_unique_vals, title='Random Forest with PCA : Confusion Matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimator :\n",
    "rf_pca_model = rf_grid_search.best_estimator_['pca']\n",
    "rf_trained_model = rf_grid_search.best_estimator_['model']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find contribution of each feature to the model\n",
    "rf_pca_weights_df = get_pca_features_weights(pca_model=rf_pca_model,\n",
    "                                            classifier_model=rf_trained_model,\n",
    "                                            feature_names=rf_X_test_pca.columns)\n",
    "if rf_pca_weights_df is not None:\n",
    "    # plot_feature_importance_comparison(pca_model, rd_model, pca_weights_df, X_pca_dataframe)\n",
    "    plot_feature_importance_comparison_plotly(rf_pca_model, rf_model, rf_pca_weights_df, rf_X_test_pca)\n",
    "else :\n",
    "    print(f'PCA Weights for {rf_trained_model} could not be calculated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_pca_weights_df[rf_pca_weights_df > 0][1:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Persisting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "joblib.dump(rf_trained_model, f\"{MODELS_DIR}RForest_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test to make sure persisted model can be reloaded correctly.\n",
    "rf_model_joblib: RandomForestClassifier = joblib.load(f\"{MODELS_DIR}RForest_model.pkl\")\n",
    "print(rf_model_joblib.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHAP :\n",
    "# plot_SHAP(pca_model=rf_pca_model, classifier_model=rf_trained_model, X_df=rf_X_test_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB requires target vars in binary format and hence cannot reuse the fn used for RandomForest\n",
    "def fit_xgb_classifier(model: XGBClassifier, model_params:dict, X_df: DataFrame, y_series:ndarray):\n",
    "    '''\n",
    "    @param model : Classifier model\n",
    "    @param model_params : Params used for tuning the hyperparameters of the given model\n",
    "    @param X_df : DataFrame of the independent vars with data\n",
    "    @param y_series : An ndarray object with binary representation of target classes\n",
    "    @return dict : Consisting of keys : Execution Time, Accuracy, Precision, Recall, F1-Score, Confusion matrix, Best Estimate, Best Params, Best Accuracy\n",
    "    '''\n",
    "    # for model_name, model_params in models_config.items():\n",
    "    start_time = time()\n",
    "\n",
    "    grid_search = create_grid_model(model=model, model_params=model_params)\n",
    "    grid_search.fit(X_df, y_series)\n",
    "\n",
    "    end_time = time()\n",
    "\n",
    "    return grid_search\n",
    "\n",
    "\n",
    "def predict_xgb_classifier(grid_search: GridSearchCV, X_df: DataFrame, y_series_bin: ndarray):\n",
    "    start_time = time()\n",
    "\n",
    "    y_preds = grid_search.predict(X_df)\n",
    "\n",
    "    end_time = time()\n",
    "\n",
    "    y_test_bin_arr = np.array(y_series_bin)\n",
    "    # confusion matrix\n",
    "    cm = confusion_matrix(y_true=np.argmax(y_test_bin_arr, axis=1), y_pred=np.argmax(y_preds, axis=1)) #, labels=class_unique_vals)\n",
    "    # plot_confusion_matrix(cm, labels=class_unique_vals, title='XGBoost with PCA : Confusion Matrix')\n",
    "\n",
    "    best_results = {\n",
    "        'Execution Time': (end_time - start_time),\n",
    "        'Accuracy': accuracy_score(y_series_bin, y_preds),\n",
    "        'Precision': precision_score(y_series_bin, y_preds, average='weighted'),\n",
    "        'Recall': recall_score(y_series_bin, y_preds,  average='weighted'),\n",
    "        'F1-Score': f1_score(y_series_bin, y_preds, average='weighted'),\n",
    "        'Confusion matrix': cm,\n",
    "        # 'GridSearchCV': grid_search,\n",
    "        'Best Estimate': grid_search.best_estimator_,\n",
    "        'Best Params': grid_search.best_params_,\n",
    "        'Best Accuracy': grid_search.best_score_\n",
    "    }\n",
    "\n",
    "    return best_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = models_config['XGBoost']['model']\n",
    "xgb_model_config = models_config['XGBoost']['params']\n",
    "\n",
    "y_train_bin = convert_to_binary(y_train)\n",
    "# print(y_train_bin)\n",
    "# Uncomment below to identify mappings between binary format and the actual label\n",
    "# 0=BRCA 1=COAD, 2=KIRC, 3=LUAD, 4=PRAD\n",
    "# print(\"Binary data: \", y_train_bin)\n",
    "# print(\"Label to int mapping: \", label_binzer.inverse_transform(np.array(y_train_bin)))\n",
    "\n",
    "xgb_grid_search = fit_xgb_classifier(model=xgb_model, model_params=xgb_model_config, X_df=X_train_pca, y_series=y_train_bin)\n",
    "\n",
    "# xgb_train_best_results = predict_xgb_classifier(grid_search=xgb_grid_search, X_df=X_train_pca, y_series_bin=y_train_bin)\n",
    "# print(xgb_train_best_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Evaluation (XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_y_test_bin = convert_to_binary(y_test)\n",
    "xgb_X_test_pca = clean_input_data(X_test, df_feature_outliers)\n",
    "\n",
    "xgb_best_results = predict_xgb_classifier(grid_search=xgb_grid_search, X_df=xgb_X_test_pca, y_series_bin=xgb_y_test_bin)\n",
    "print(xgb_best_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_conf_matrix = xgb_best_results['Confusion matrix']\n",
    "plot_confusion_matrix(xgb_conf_matrix, labels=class_unique_vals, title='XGBoost with PCA : Confusion Matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pca_model = xgb_grid_search.best_estimator_['pca']\n",
    "xgb_trained_model = xgb_grid_search.best_estimator_['model']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find contribution of each feature to the model\n",
    "xgb_pca_weights_df = get_pca_features_weights(pca_model=xgb_pca_model,\n",
    "                                            classifier_model=xgb_trained_model,\n",
    "                                            feature_names=xgb_X_test_pca.columns)\n",
    "if xgb_pca_weights_df is not None:\n",
    "    # plot_feature_importance_comparison(pca_model, rd_model, pca_weights_df, X_pca_dataframe)\n",
    "    plot_feature_importance_comparison_plotly(xgb_pca_model, xgb_trained_model, xgb_pca_weights_df, xgb_X_test_pca)\n",
    "else :\n",
    "    print(f'PCA Weights for {rf_trained_model} could not be calculated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persisting data for tests\n",
    "# print(xgb_X_test_pca.head())\n",
    "xgb_X_test_pca.to_csv(DATA_DIR + 'xgb_X_after_pca_dataset.csv')\n",
    "print('516: ', y_test[516])\n",
    "print('329: ', y_test[329])\n",
    "print('52: ', y_test[52])\n",
    "print('141: ', y_test[141])\n",
    "\n",
    "\n",
    "\n",
    "# Generate statistical description of these columns to help generate random values on the App side - deprecated\n",
    "len(xgb_X_test_pca.columns)\n",
    "xgb_test_descr_df = xgb_X_test_pca.describe().T\n",
    "# xgb_test_descr_df.to_csv(DATA_DIR + 'xgb_test_X_describe.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pca_model.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Persisting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist just the XGBooster model. If used the inputs would be 640 PCA components which can be identified using xgb_pca_model.get_feature_names_out()\n",
    "joblib.dump(xgb_trained_model, f\"{MODELS_DIR}XGBoost_model.pkl\")\n",
    "\n",
    "# persis the whole trained GridSearchCV\n",
    "joblib.dump(xgb_grid_search, f\"{MODELS_DIR}xgb_GridSearch_Pipeline.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test to make sure persisted model can be reloaded correctly.\n",
    "# xgb_model_joblib: XGBClassifier = joblib.load(f\"{MODELS_DIR}XGBoost_model.pkl\")\n",
    "# print(xgb_model_joblib.feature_importances_)\n",
    "\n",
    "xgb_gridcv_pipeline_joblib: GridSearchCV = joblib.load(f\"{MODELS_DIR}xgb_GridSearch_Pipeline.pkl\")\n",
    "xgb_gridcv_pipeline_joblib.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit testing : to make sure model prediction is same as when input from Web App\n",
    "unittest_data_X = pd.read_csv(DATA_ANALYSIS_DIR + 'request.csv')\n",
    "# print(\"Printing req data: \\n\", type(unittest_data_X.iloc[0:1, 1:]))\n",
    "unittest_data_pred = xgb_grid_search.predict(unittest_data_X.iloc[0:1, 1:])\n",
    "\n",
    "# unittest_data_pred = xgb_grid_search.predict(xgb_X_test_pca.iloc[0:1, 0:])\n",
    "unittest_data_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the 2 Classifier Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame([rf_best_results, xgb_best_results]).T #.sort_values(by='Accuracy', ascending=False)\n",
    "results_df.columns = ['Random Forest', 'XGBoost']\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Under Construction\n",
    "Please ignore below section as this code is being worked on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area under Curve - REF : https://www.geeksforgeeks.org/interpreting-random-forest-classification-results/\n",
    "# -------- #TODO : need to fix the roc_curve function\n",
    "def plot_aoc_randomforest(rd_test_pred_proba: ndarray, y_test: Series) :\n",
    "    target_vals = y_test.unique()\n",
    "    # y_test_bin = label_binarize(y_test, classes=[0,1,2,3,4])\n",
    "    label_binzer = LabelBinarizer()\n",
    "    label_binzer.fit(y_test)\n",
    "    y_test_bin = np.array(label_binzer.transform(y_test))\n",
    "    print(y_test_bin[0])\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    print(f\"y_test_bin : {y_test_bin[1]}\")\n",
    "    print(f\"rd_preds_prob : {rd_test_pred_proba[:, 1]}\")\n",
    "    # print(f\"y_test_bin : {len(y_test_bin[:, 1])}\")\n",
    "    # print(f\"rd_preds_prob : {rd_preds_prob[:, 1]}\")\n",
    "    # print(rd_preds_prob)\n",
    "\n",
    "    for index in range(len(target_vals)):\n",
    "        fpr[index], tpr[index], _ = roc_curve(y_test_bin[index], rd_test_pred_proba[:, index])\n",
    "        # print(f\"FPR at {index}: \\n{fpr[index]}\")\n",
    "        # print(f\"TPR at {index}: \\n{tpr[index]}\")\n",
    "        roc_auc[index] = auc(fpr[index], tpr[index])\n",
    "\n",
    "    # Plot ROC curve\n",
    "    # plt.figure()\n",
    "    # for index in range(len(target_vals)) :\n",
    "    #     plt.plot(fpr[index], tpr[index], lw=2, label=f\"ROC curve of class {target_vals[index]} (area = {roc_auc[index]:.2f})\")\n",
    "\n",
    "    # # plt.plot([0,1], [0,1], color='navy', lw=2, linestyle='--')\n",
    "    # plt.xlim([0.0, 1.0])\n",
    "    # plt.ylim([0.0, 1.05])\n",
    "    # plt.xlabel('False Positive Rate')\n",
    "    # plt.ylabel('True Positive Rate')\n",
    "    # plt.title('Receiver Operating Characterstic for Tumor classes')\n",
    "    # plt.legend(loc=\"lower right\")\n",
    "    # plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
