{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "470d343b",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f1ed90",
   "metadata": {},
   "source": [
    "This project tries to identify the sentiment of a mental health patient based on patient's description of current feelings.\n",
    "\n",
    "Data is sourced from https://www.kaggle.com/datasets/thedevastator/nlp-mental-health-conversations/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9972056b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opendatasets --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3f94d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "# We have to check which Torch version for Xformers (2.3 -> 0.0.27)\n",
    "# from torch import __version__; from packaging.version import Version as V\n",
    "# xformers = \"xformers==0.0.27\" if V(__version__) < V(\"2.4.0\") else \"xformers\"\n",
    "# !pip install --no-deps {xformers} trl peft accelerate bitsandbytes triton\n",
    "!pip install triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cff725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import opendatasets as od \n",
    "import pandas as pd \n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef38246",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b4f014",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef94943e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "DATA_DIR = '../../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0858041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = 'https://www.kaggle.com/datasets/thedevastator/nlp-mental-health-conversations/data'\n",
    "od.download(dataset_url, data_dir=DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf30503",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_DATA_DIR = DATA_DIR + '/nlp-mental-health-conversations'\n",
    "df = pd.read_csv(CSV_DATA_DIR + '/train.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d02b541",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f745b17b",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8db0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0604bab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for any odd text\n",
    "zero_indexes = df[df['Response'] == '0'].index.tolist()\n",
    "print(zero_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99227fe0",
   "metadata": {},
   "source": [
    "### Data Cleansing and Pre-Processing\n",
    "\n",
    "**NOTE** : We are using a pre-trained model and hence the need to split data into train, test, validate isnt needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b73ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given we have 4 records with NaNs and 1 record without a proper value, out of 3512 records we shall drop these records\n",
    "df_clean = df.dropna()\n",
    "df_clean = df_clean.drop(index=zero_indexes)\n",
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61f412e",
   "metadata": {},
   "source": [
    "## Univariate and Multivariate Analysis\n",
    "\n",
    "### Statistical Description\n",
    "### Correlation\n",
    "### Outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c11553",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c5c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "\n",
    "from transformers import pipeline\n",
    "from torch import device, mode\n",
    "import textwrap\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "# Downlaod the averaged_perceptron_tagger_eng package\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e27c6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for better lemmatization - word is normalized to its grammatical foundation word\n",
    "from curses.ascii import isalpha\n",
    "import token\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun\n",
    "\n",
    "\n",
    "def lemmitize(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    lemmatized = [\n",
    "        lemmatizer.lemmatize(token, get_wordnet_pos(pos))\n",
    "        for token, pos in tagged_tokens\n",
    "        if token.isalpha()\n",
    "    ]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "# Quick test\n",
    "# sentence = '''I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\n",
    "#    I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it.\n",
    "#    How can I change my feeling of being worthless to everyone?'''\n",
    "\n",
    "# lemmitize(sentence)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ac1232",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model_pipeline() :\n",
    "    # REF: https://stackoverflow.com/a/75499889\n",
    "    model_name = \"sid321axn/Bio_ClinicalBERT-finetuned-medicalcondition\"\n",
    "\n",
    "    # using auto tokenizer - str to tokens (numbers)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # sentiment_model = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "    classifier_pipe = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=model_name,\n",
    "        tokenizer=tokenizer,\n",
    "        device=0 if torch.cuda.is_available() else -1\n",
    "    )\n",
    "\n",
    "    return classifier_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ded6a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# break up long sentences\n",
    "# TODO: Once we lemmitize, the sentence loses all periods in it. Without periods its difficult to ensure the context of the \n",
    "#   orig sentence is retained. Need to figure out a better way\n",
    "# def split_long_sentences(text: str, batch_size: int= 512) -> List[str]:\n",
    "#     words = text.split()\n",
    "#     chunks = []\n",
    "#     i = 0\n",
    "#     while (i < len(words)):\n",
    "#         words_batch = words[i: (i+batch_size)]\n",
    "#         sentence_batch = ' '.join(words_batch)\n",
    "#         chunks.append(sentence_batch)\n",
    "\n",
    "#         i = i+batch_size\n",
    "\n",
    "#     return chunks\n",
    "\n",
    "# # Quick test\n",
    "# long_text:str = '''I'm a teenager, and throughout my entire life, I've never really had good parents, or parents at all for that matter. I'm not exaggerating. I was living with my mom and grandparents until my father, in prison for most of my life, got out when I was in middle school. His mom, my grandma, only lived a mile down the road from my mom’s house, and I was so awe-stricken with my dad that I got to stay with him for a long time. Meanwhile, I did not realize that my mom was doing hardcore drugs. My mom went to prison for that and lost her café. We live in a very small town, so everyone knew about it, and I was bullied because of who my parents were. My dad ended up getting in with the wrong people and went back to prison. My mom and him had a mutual friend and often hung out at that person’s house. My parents did not get along at this point. We were driving him home one day from this house, and my mom stopped the car and kicked him out. He got out of the car, went to the driver’s side, and punched my mom in the face. I got out and told him not to hit my mom. At that point, I was really scared and mad that he did that, so I ran towards him to stop him. He literally picked me up and threw me on the back of a gravel road. I couldn't even walk. My mom tried to help me, but he started choking her. I hobbled over, and she barely got into the car, and we quickly drove away and called the police and ambulance. He was so badly strung out on drugs. He went to prison again and seems to be doing well. I met up with him once with my grandma, and we had coffee, but he's so hard to handle. I think a lot of it is that I can't bring myself to forgive him. My mom went back to prison again for drugs, and while she was in there, I moved in with my dad’s mom (the one who lived just down the road) because I trust her, her house is stable, and she's more nurturing, understanding, and loving then my other grandparents. I also stay at my boyfriend’s a lot. Now that my mom is out of prison, she's trying to control every aspect of my life. She’s trying to make me move back home out of Susan's house, and I don't want to. I don't like it there. They condone drug abuse and many other things, and I'm just not comfortable. She's even threatened to call the police and say I'm a runaway because she has custody of me. My boyfriend has always had this picture-perfect life, and his family are strict Christians. One time, his mom even went as far as to say that if he and I break up, if we were having sex, I would say that he\", \"raped me. I've got so many problems I don't even know what to do.'''\n",
    "# chunked_text:List = split_long_sentences(long_text)\n",
    "# index=0\n",
    "# for t in chunked_text:\n",
    "#     print(f\"Sentence at {index} \", t)\n",
    "#     index +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd44740",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ref: Google Gemini - \"how to shorten a long text without losing its context\"\n",
    "def summarize_text(text: str, num_sentences: int) -> str:\n",
    "    sentences = sent_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_frequencies = defaultdict(int) # \n",
    "\n",
    "    # In each sentence within txt, remove stop words and numbers, get each word frequency\n",
    "    for sentence in sentences:\n",
    "        for word in sentence.lower().split():\n",
    "            if word.isalpha() and word not in stop_words:\n",
    "                word_frequencies[word] += 1\n",
    "\n",
    "    # scaling freqs to 1 (Min-Max scaling)\n",
    "    maximum_frequency = max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = (word_frequencies[word] / maximum_frequency)\n",
    "    \n",
    "    # calc score for each sentence based on word freq\n",
    "    sentence_scores = defaultdict(int)\n",
    "    for sentence in sentences:\n",
    "        for word in sentence.lower().split():\n",
    "            if word.isalpha() and word in word_frequencies:\n",
    "                sentence_scores[sentence] += word_frequencies[word]\n",
    "                # print(\"sentence score: \", sentence_scores)\n",
    "\n",
    "    summary_sentences = heapq.nlargest(num_sentences, sentence_scores, key=sentence_scores.get)\n",
    "    return ' '.join(summary_sentences)\n",
    "\n",
    "# Quick test \n",
    "long_text:str = '''I'm a teenager, and throughout my entire life, I've never really had good parents, or parents at all for that matter. I'm not exaggerating. I was living with my mom and grandparents until my father, in prison for most of my life, got out when I was in middle school. His mom, my grandma, only lived a mile down the road from my mom’s house, and I was so awe-stricken with my dad that I got to stay with him for a long time. \n",
    "Meanwhile, I did not realize that my mom was doing hardcore drugs. My mom went to prison for that and lost her café. We live in a very small town, so everyone knew about it, and I was bullied because of who my parents were. My dad ended up getting in with the wrong people and went back to prison. My mom and him had a mutual friend and often hung out at that person’s house. \n",
    "My parents did not get along at this point. We were driving him home one day from this house, and my mom stopped the car and kicked him out. He got out of the car, went to the driver’s side, and punched my mom in the face. I got out and told him not to hit my mom. At that point, I was really scared and mad that he did that, so I ran towards him to stop him. He literally picked me up and threw me on the back of a gravel road. \n",
    "I couldn't even walk. My mom tried to help me, but he started choking her. I hobbled over, and she barely got into the car, and we quickly drove away and called the police and ambulance. He was so badly strung out on drugs. He went to prison again and seems to be doing well. I met up with him once with my grandma, and we had coffee, but he's so hard to handle. I think a lot of it is that I can't bring myself to forgive him. \n",
    "My mom went back to prison again for drugs, and while she was in there, I moved in with my dad’s mom (the one who lived just down the road) because I trust her, her house is stable, and she's more nurturing, understanding, and loving then my other grandparents. I also stay at my boyfriend’s a lot. Now that my mom is out of prison, she's trying to control every aspect of my life. \n",
    "She’s trying to make me move back home out of Susan's house, and I don't want to. I don't like it there. They condone drug abuse and many other things, and I'm just not comfortable. She's even threatened to call the police and say I'm a runaway because she has custody of me. My boyfriend has always had this picture-perfect life, and his family are strict Christians. \n",
    "One time, his mom even went as far as to say that if he and I break up, if we were having sex, I would say that he\", \"raped me. I've got so many problems I don't even know what to do.'''\n",
    "summ_text:str = summarize_text(long_text, 3)\n",
    "print('summarized text: ', summ_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacabc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_pipeline = get_model_pipeline()\n",
    "def get_sentiment(sentence:str):\n",
    "    lem_sentence = sentence # lemmitize(sentence)\n",
    "    print(lem_sentence)\n",
    "    sentiment = cls_pipeline(lem_sentence)\n",
    "\n",
    "    # print(f\"Sentiment for index {index}: Label={sentiment[0]['label']}, Score={sentiment[0]['score']} \")\n",
    "    return {\n",
    "        'Sentence': sentence,\n",
    "        'Label': sentiment[0]['label'],\n",
    "        'Confidence': sentiment[0]['score']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceed9d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO parametrize this (max # of tokens a model supports) at the model level above\n",
    "batch_size = 512\n",
    "num_sentences = 3\n",
    "\n",
    "def update_with_sentiment(row):\n",
    "    context_text = row['Context']\n",
    "    # for now summarizing text only if sentence goes over the # of tokens the model is trained for. \n",
    "    if len(context_text.split()) > batch_size:\n",
    "        context_text = summarize_text(context_text, num_sentences=num_sentences)\n",
    "        row['SummarizedText'] = context_text\n",
    "\n",
    "    context_text = lemmitize(context_text)\n",
    "    sentiment = get_sentiment(context_text)\n",
    "    print(f\"Sentiment for : Label={sentiment['Label']}, Confidence={sentiment['Confidence']} \")\n",
    "    row['Label'] = sentiment['Label']\n",
    "    row['Confidence'] = sentiment['Confidence']\n",
    "\n",
    "    return row\n",
    "    \n",
    "df_sent = df_clean.apply(update_with_sentiment, axis=1)\n",
    "df_sent.head()\n",
    "\n",
    "# for index, row in df_clean.iterrows():\n",
    "#     context_text = row['Context']\n",
    "#     # for now summarizing text only if sentence goes over the # of tokens the model is trained for. \n",
    "#     if len(context_text.split()) > batch_size:\n",
    "#         context_text = summarize_text(context_text, num_sentences=num_sentences)\n",
    "#         df_clean[index]['SummarizedText'] = context_text\n",
    "\n",
    "#     context_text = lemmitize(context_text)\n",
    "#     sentiment = get_sentiment(context_text)\n",
    "#     print(f\"Sentiment for index {index}: Label={sentiment['Label']}, Confidence={sentiment['Confidence']} \")\n",
    "#     df_clean['Label']\n",
    "    \n",
    "    # if len(context_text.split()) > 512:\n",
    "    #     print(\"........................................\")\n",
    "    #     # process as batch given some of the sentences are longer than supported by the model that is trained for a tensor size of 512\n",
    "    #     sentence_list = split_long_sentences(context_text)\n",
    "\n",
    "    #     for sntc in sentence_list:\n",
    "    #         print(\"list of sentences: \", sentence_list)\n",
    "    #         sentiment = get_sentiment(sntc)\n",
    "    #         print(f\"Sentiment for index {index}: Label={sentiment['Label']}, Score={sentiment['Score']} \")\n",
    "\n",
    "    # else :\n",
    "    #     sentiment = get_sentiment(context_text)\n",
    "    #     print(f\"Sentiment for index {index}: Label={sentiment['Label']}, Score={sentiment['Score']} \")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc175c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_order = ['Context', 'Response', 'SummarizedText', 'Label', 'Confidence']\n",
    "df_sent = df_sent[cols_order]\n",
    "df_sent.to_csv(f\"{DATA_DIR}/output/mental_health_convo_categ.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba34c97b",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a68753",
   "metadata": {},
   "source": [
    "### Persist Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
