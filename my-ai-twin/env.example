# OpenAI API config
OPENAI_MODEL_ID=gpt-4o-mini
OPENAI_API_KEY=

# Huggingface API config
HUGGINGFACE_ACCESS_TOKEN=

# Comet ML (during training and inference)
COMET_API_KEY=
# such as your Comet username
COMET_WORKSPACE=

# --- Required settings ONLY when using Qdrant Cloud and AWS SageMaker ---

# Qdrant cloud vector database connection config - Optional only if using cloud version
USE_QDRANT_CLOUD=false
QDRANT_CLOUD_URL=
QDRANT_APIKEY=

# AWS authentication config - Only when deploying to AWS. Not reqd for local run.
AWS_ARN_ROLE=
AWS_REGION=us-east-1
AWS_ACCESS_KEY=
AWS_SECRET_KEY=
AWS_LAMBDA_FUNCTION_TIMEOUT=900
# needed for debug
AWS_LAMBDA_RUNTIME_API=

# Opik config
OPIK_API_KEY=
OPIK_URL_OVERRIDE="https://www.comet.com/opik/api"
OPIK_PROJECT_NAME="my-ai-twin"
# OPIK_WORKSPACE="your-workspace-name"
OPIK_FILE_LOGGING_LEVEL="DEBUG"

# To address the huggingface/tokenizer issue : Disable parallelism in this given we use parallel
# processes in vector_retriever.py. Refer to the content on the error "huggingface/tokenizers: The
# current process just got forked, after parallelism has already been used. Disabling parallelism
# to avoid deadlocks..."
TOKENIZERS_PARALLELISM=false


# RabbitMQ
RABBITMQ_DEFAULT_USERNAME=
RABBITMQ_DEFAULT_PASSWORD=